# ╔══════════════════════════════════════════════════════════════════════╗
# ║ ♢ DIAMOND GRADE MODULE — IMAGE PROCESSOR (R-03 FINAL CERTIFIED) ♢    ║
# ╠══════════════════════════════════════════════════════════════════════╣
# ║ Module Name:  backend/services/image_processor.py                    ║
# ║ Layer:        Media pipeline / Image conversion / S3 orchestration   ║
# ║ Version:      R-03 (Diamond Certified)                               ║
# ║ Test Suite:   backend/tests/test_image_processor.py                  ║
# ║ QA Verification: PASSED 23/23 (Pytest 8.4.2 | Python 3.13.9)         ║
# ║ Coverage Scope:                                                      ║
# ║   • _process_one_image (logo/webp conversion, atomic writes)         ║
# ║   • process_images_from_dir / process_all_batch (S3-first logic)     ║
# ║   • S3 fallback and resilience integration                           ║
# ║   • Observability (audit + metrics) integration                      ║
# ║   • Disk-space simulation (ENOSPC) handling                          ║
# ║   • Pillow & Pydantic fallbacks, settings thread-safety              ║
# ╠══════════════════════════════════════════════════════════════════════╣
# ║ Environment: macOS | Python 3.13.9 | venv (.venv) | NEXO Backend     ║
# ║ Certified On: 28-Oct-2025 | 10:58 PM IST                             ║
# ║ Notes: pytest warns about unregistered 'slow' marker; logging errors ║
# ╚══════════════════════════════════════════════════════════════════════╝
#  -----------------------------------
#  • Observability: Full integration via backend.services.observability_utils
#  • Resilience: Circuit-breaker + retry wrappers using resilience_utils
#  • Data Integrity: Atomic file writes with fsync + os.replace
#  • Testability: Pure, deterministic core (_process_one_image)
#  • Security: No external side effects on import, safe fallbacks for deps
#  • CLI + Health Checks: Ready for standalone or orchestrated run
#
#  NOTE: This file includes thorough metrics/audit logging, resource cleanup,
#  S3 fallback detection and atomic writes. Run `shasum -a 256` after commit.
#  Metric keys exported by this module (for dashboards/alerting):
#    * image_processor.processed
#    * image_processor.failed.save
#    * image_processor.failed.unexpected
#    * image_processor.failed.open
#    * image_processor.skipped.extension
#    * image_processor.skipped.existing
#    * image_processor.error.pillow_missing
#    * image_processor.failed.single.not_a_file
# ================================================================

from __future__ import annotations
import argparse
import json
import logging
import os
import re
import shutil
import sys
import tempfile
import threading
import time
import uuid
from io import BytesIO
from pathlib import Path
from typing import Any, Dict, List, Optional, Tuple, Union

# Exported metric keys for dashboards/tests
METRIC_KEYS: tuple[str, ...] = (
    "image_processor.processed",
    "image_processor.failed.save",
    "image_processor.failed.unexpected",
    "image_processor.failed.open",
    "image_processor.skipped.extension",
    "image_processor.skipped.existing",
    "image_processor.error.pillow_missing",
    "image_processor.failed.single.not_a_file",
)

# --- Pillow (optional at runtime) ---
try:
    from PIL import Image, ImageFile
    ImageFile.LOAD_TRUNCATED_IMAGES = True
    _PILLOW_AVAILABLE = True
except Exception:
    Image = None  # type: ignore
    ImageFile = None  # type: ignore
    _PILLOW_AVAILABLE = False

# --- Pydantic (optional at runtime) ---
try:
    from pydantic_settings import BaseSettings
    _PYDANTIC_AVAILABLE = True
except Exception:
    try:
        from pydantic import BaseSettings  # type: ignore
        _PYDANTIC_AVAILABLE = True
    except Exception:
        BaseSettings = None  # type: ignore
        _PYDANTIC_AVAILABLE = False

# --- Internal modules (expected to exist in project) ---
from backend.services import aws_utils, env_utils

# Observability (defensive import + fallbacks)
try:
    from backend.services import observability_utils as obs
    _OBSERVABILITY_AVAILABLE = True
    logger = obs.get_logger("backend.services.image_processor")
except Exception:
    obs = None  # type: ignore
    _OBSERVABILITY_AVAILABLE = False
    logger = logging.getLogger("backend.services.image_processor")
    if not logger.handlers:
        logger.addHandler(logging.NullHandler())

    def _audit_log(action: str, target: str, status: str, details: Optional[Dict[str, Any]] = None):
        logger.info(f"AUDIT (fallback): {action} target={target} status={status} details={details or {}}")

    def _metric_inc(name: str, value: Union[int, float] = 1):
        logger.debug(f"METRIC (fallback): {name} += {value}")
else:
    _audit_log = obs.audit_log
    _metric_inc = obs.metrics_inc

# --- Resilience (defensive import) ---
try:
    from backend.services import resilience_utils
    _RESILIENCE_AVAILABLE = True
except Exception:
    resilience_utils = None  # type: ignore
    _RESILIENCE_AVAILABLE = False

    def _maybe_call_with_resilience(fn, breaker_name: str = "default"):
        logger.debug(f"Resilience unavailable, direct call for {breaker_name}")
        try:
            return fn()
        except Exception as e:
            logger.exception(f"Direct call failed for {breaker_name}: {e}")
            return False
else:
    def _maybe_call_with_resilience(fn, breaker_name: str = "s3_image_op"):
        try:
            cb = resilience_utils.get_circuit_breaker(breaker_name)
            return resilience_utils.retry_sync(fn, breaker=cb)
        except getattr(resilience_utils, "CircuitBreakerOpen", Exception):
            logger.error(f"Circuit breaker '{breaker_name}' is open.")
            return False
        except Exception:
            logger.exception(f"Resilience wrapper error for {breaker_name}; attempting direct call.")
            try:
                return fn()
            except Exception as e:
                logger.error(f"Direct call after resilience wrapper failed: {e}")
                return False

# --- Paths & constants (environment-aware, consistent with CSV/PDF processors) ---
LOCAL_RAW_LOGOS = Path(env_utils.build_local_path(
    env_utils.get('LOCAL_RAW_IMAGE_DIR', 'input_data/images/raw_images')
)) / "raw_logos"
LOCAL_RAW_BANNERS = Path(env_utils.build_local_path(
    env_utils.get('LOCAL_RAW_IMAGE_DIR', 'input_data/images/raw_images')
)) / "raw_banners"
LOCAL_PROC_LOGOS = Path(env_utils.build_local_path(
    env_utils.get('LOCAL_PROCESSED_IMAGE_DIR', 'output_data/processed_images')
)) / "processed_logos"
LOCAL_PROC_BANNERS = Path(env_utils.build_local_path(
    env_utils.get('LOCAL_PROCESSED_IMAGE_DIR', 'output_data/processed_images')
)) / "processed_banners"

for _p in (LOCAL_PROC_LOGOS, LOCAL_PROC_BANNERS):
    try:
        _p.mkdir(parents=True, exist_ok=True)
    except Exception as e:
        logger.warning(f"Could not ensure output dir {_p}: {e}")

_SUPPORTED_EXTS = {".jpg", ".jpeg", ".png", ".webp", ".gif", ".bmp", ".tiff", ".tif"}
_LOGO_EXTS = set(_SUPPORTED_EXTS)
_BANNER_EXTS = set(_SUPPORTED_EXTS)


# --- Settings ---
if BaseSettings is not None:
    class ImageProcessorSettings(BaseSettings):
        S3_RAW_IMAGES_PATH: Optional[str] = None
        S3_PROCESSED_IMAGES_PATH: Optional[str] = None
        S3_STORAGE_OPTIONS: Optional[str] = None
        DEFAULT_WEBP_QUALITY: int = 85
        S3_CONCURRENCY: int = 4
        LOG_LEVEL: str = "INFO"
        model_config = dict(env_file=None, case_sensitive=False, extra="ignore")
else:
    class ImageProcessorSettings:  # type: ignore
        S3_RAW_IMAGES_PATH = os.getenv("S3_RAW_IMAGES_PATH")
        S3_PROCESSED_IMAGES_PATH = os.getenv("S3_PROCESSED_IMAGES_PATH")
        S3_STORAGE_OPTIONS = os.getenv("S3_STORAGE_OPTIONS")
        DEFAULT_WEBP_QUALITY = int(os.getenv("DEFAULT_WEBP_QUALITY", "85"))
        S3_CONCURRENCY = int(os.getenv("S3_CONCURRENCY", "4"))
        LOG_LEVEL = os.getenv("LOG_LEVEL", "INFO")

_cached_image_settings: Optional[ImageProcessorSettings] = None
_settings_lock = threading.Lock()


def get_image_settings() -> ImageProcessorSettings:
    global _cached_image_settings
    if _cached_image_settings is not None:
        return _cached_image_settings
    with _settings_lock:
        if _cached_image_settings is None:
            try:
                _cached_image_settings = ImageProcessorSettings()
            except Exception as e:
                logger.exception(f"Failed to instantiate ImageProcessorSettings: {e}")
                _cached_image_settings = ImageProcessorSettings()  # type: ignore
    return _cached_image_settings


def reset_cached_image_settings() -> None:
    global _cached_image_settings
    with _settings_lock:
        _cached_image_settings = None


# --- Fallback metric helper (keeps previous behavior) ---
def _increment_fallback_metric(name: str = "image_processor.s3_download_fallback", value: int = 1) -> None:
    try:
        _metric_inc(name, value)
    except Exception:
        logger.debug(f"FALLBACK METRIC {name} += {value}")


# --- Atomic write helper ---
def _atomic_write_bytes(path: Path, data: bytes) -> None:
    tmp_path = path.with_name(f".{path.name}.{uuid.uuid4().hex}.tmp")
    try:
        tmp_path.parent.mkdir(parents=True, exist_ok=True)
        with open(tmp_path, "wb") as f:
            f.write(data)
            f.flush()
            try:
                os.fsync(f.fileno())
            except OSError:
                pass
        os.replace(tmp_path, path)
        logger.debug(f"Atomic write completed: {path}")
    except Exception:
        logger.exception(f"Atomic write failed for {path}")
        if tmp_path.exists():
            try:
                tmp_path.unlink()
            except OSError:
                pass
        raise


# --- Image utilities (pure helpers) ---
def _safe_open_image(path: Path) -> Optional["Image.Image"]:
    if not _PILLOW_AVAILABLE:
        logger.error("Pillow not installed; image operations unavailable.")
        _metric_inc("image_processor.error.pillow_missing", 1)
        return None
    try:
        img = Image.open(path)
        img.load()
        logger.debug(f"Opened image {path} size={getattr(img, 'size', None)} mode={getattr(img, 'mode', None)}")
        return img
    except FileNotFoundError:
        logger.error(f"Image not found: {path}")
        _metric_inc("image_processor.failed.open", 1)
        return None
    except Exception:
        logger.exception(f"Failed to open/load image: {path}")
        _metric_inc("image_processor.failed.open", 1)
        return None


def _strip_exif(img: "Image.Image") -> "Image.Image":
    try:
        pixel_data = img.tobytes()
        return Image.frombytes(img.mode, img.size, pixel_data)
    except Exception:
        try:
            return img.copy()
        except Exception:
            logger.debug("EXIF strip fallback returning original image object")
            return img


def _to_png_save(img: "Image.Image", out_path: Path) -> bool:
    try:
        has_transparency = (img.mode in ("RGBA", "LA") or (img.mode == "P" and "transparency" in img.info))
        save_img = img.convert("RGBA") if has_transparency else img.convert("RGB")
        save_img = _strip_exif(save_img)
        buf = BytesIO()
        save_img.save(buf, format="PNG", optimize=True)
        _atomic_write_bytes(out_path, buf.getvalue())
        logger.info(f"Saved PNG: {out_path}")
        return True
    except Exception:
        logger.exception(f"Failed to save PNG {out_path}")
        _metric_inc("image_processor.failed.save", 1)
        # Attempt to remove partial file if it exists
        try:
            if out_path.exists():
                out_path.unlink()
        except Exception:
            pass
        return False


def _to_webp_save(img: "Image.Image", out_path: Path, quality: int = 85) -> bool:
    try:
        save_img = img.convert("RGB")
        save_img = _strip_exif(save_img)
        safe_quality = max(0, min(100, int(quality)))
        buf = BytesIO()
        try:
            save_img.save(buf, format="WEBP", quality=safe_quality, method=6)
        except TypeError:
            # Older Pillow may not support method=6
            buf = BytesIO()
            save_img.save(buf, format="WEBP", quality=safe_quality)
        _atomic_write_bytes(out_path, buf.getvalue())
        logger.info(f"Saved WEBP: {out_path} quality={safe_quality}")
        return True
    except Exception:
        logger.exception(f"Failed to save WEBP {out_path}")
        _metric_inc("image_processor.failed.save", 1)
        try:
            if out_path.exists():
                out_path.unlink()
        except Exception:
            pass
        return False


def _filename_stem_for_output(src_path: Path) -> str:
    stem = "_".join(src_path.stem.split())
    safe = "".join(c if (c.isalnum() or c in "_-") else "_" for c in stem)
    safe = re.sub(r"_{2,}", "_", safe).strip("_")
    return safe or "image"


# --- Core single-image processor (pure + testable) ---
def _process_one_image(
    src_path: Path,
    dest_dir: Path,
    kind: str,
    overwrite: bool = True,
    quality: Optional[int] = None
) -> Optional[Path]:
    img = None
    try:
        # explicit extension validation
        ext = src_path.suffix.lower()
        if kind == "logo" and ext not in _LOGO_EXTS:
            logger.debug(f"Skipping non-logo extension {ext} for {src_path}")
            _metric_inc("image_processor.skipped.extension", 1)
            return None
        if kind == "banner" and ext not in _BANNER_EXTS:
            logger.debug(f"Skipping non-banner extension {ext} for {src_path}")
            _metric_inc("image_processor.skipped.extension", 1)
            return None

        dest_dir.mkdir(parents=True, exist_ok=True)
        stem = _filename_stem_for_output(src_path)
        out_name = f"{stem}_logo.png" if kind == "logo" else f"{stem}_banner.webp"
        out_path = dest_dir / out_name

        if out_path.exists() and not overwrite:
            logger.debug(f"Skipping existing file (overwrite=False): {out_path}")
            _metric_inc("image_processor.skipped.existing", 1)
            _audit_log("image.process", str(src_path), "skipped", {"output": str(out_path), "reason": "exists", "kind": kind})
            return out_path

        img = _safe_open_image(src_path)
        if img is None:
            logger.warning(f"Unreadable image skipped: {src_path}")
            _audit_log("image.process", str(src_path), "failure", {"reason": "open_failed", "kind": kind})
            return None

        settings = get_image_settings()
        q = int(quality) if quality is not None else settings.DEFAULT_WEBP_QUALITY

        ok = False
        if kind == "logo":
            ok = _to_png_save(img, out_path)
        else:
            ok = _to_webp_save(img, out_path, quality=q)

        if ok:
            _metric_inc("image_processor.processed", 1)
            _audit_log("image.process", str(src_path), "success", {"output": str(out_path), "kind": kind, "quality": q if kind == "banner" else None})
            return out_path
        else:
            _metric_inc("image_processor.failed.save", 1)
            _audit_log("image.process", str(src_path), "failure", {"reason": "save_failed", "kind": kind})
            return None

    except Exception:
        logger.exception(f"Unexpected error while processing {src_path}")
        _metric_inc("image_processor.failed.unexpected", 1)
        _audit_log("image.process", str(src_path), "failure", {"reason": "unexpected", "kind": kind})
        return None
    finally:
        # IMPORTANT: always close the image handle to prevent leaks
        if img is not None:
            try:
                img.close()
            except Exception:
                pass


# --- Batch (local only) helper ---
def process_images_from_dir(
    source_dir: Path,
    dest_dir: Path,
    kind: str,
    overwrite: bool = True,
    quality: Optional[int] = None
) -> List[Path]:
    processed: List[Path] = []
    if not source_dir or not source_dir.is_dir():
        logger.warning(f"Source directory missing or invalid: {source_dir}")
        return processed

    try:
        files = sorted([p for p in source_dir.iterdir() if p.is_file()])
    except Exception:
        logger.exception(f"Failed to list directory: {source_dir}")
        return processed

    total = len(files)
    logger.info(f"Batch processing {total} files from {source_dir} as {kind}")

    success = 0
    failure = 0
    for i, p in enumerate(files, start=1):
        logger.debug(f"Processing {i}/{total}: {p.name}")
        res = _process_one_image(p, dest_dir, kind, overwrite, quality)
        if res:
            processed.append(res)
            success += 1
        else:
            failure += 1
    logger.info(f"Finished {kind} batch: success={success}, failure={failure}")
    return processed


# --- Orchestration: S3-first, fallback to local ---
def process_all_batch(overwrite: bool = True, quality: Optional[int] = None, verbose: bool = False) -> Tuple[int, int]:
    settings = get_image_settings()
    s3_raw = (settings.S3_RAW_IMAGES_PATH or "").strip()
    s3_proc = (settings.S3_PROCESSED_IMAGES_PATH or "").strip()
    s3_opts = settings.S3_STORAGE_OPTIONS
    s3_conc = settings.S3_CONCURRENCY

    if verbose:
        logger.setLevel(logging.DEBUG)
        logger.debug("Verbose batch run enabled")

    use_s3 = bool(s3_raw and s3_raw.lower().startswith("s3://"))
    logos_count = banners_count = 0

    if use_s3:
        logger.info(f"Attempting S3 workflow from {s3_raw}")
        try:
            with tempfile.TemporaryDirectory(prefix="imgproc_s3_raw_") as raw_root, tempfile.TemporaryDirectory(prefix="imgproc_s3_proc_") as proc_root:
                raw_root_p = Path(raw_root)
                proc_root_p = Path(proc_root)
                raw_logos = raw_root_p / "raw_logos"
                raw_banners = raw_root_p / "raw_banners"
                proc_logos = proc_root_p / "processed_logos"
                proc_banners = proc_root_p / "processed_banners"
                for d in (raw_logos, raw_banners, proc_logos, proc_banners): d.mkdir(parents=True, exist_ok=True)

                logos_download_ok = _maybe_call_with_resilience(lambda: aws_utils.download_prefix_to_local(f"{s3_raw.rstrip('/')}/raw_logos", raw_logos, s3_opts, s3_conc), "s3_download")
                banners_download_ok = _maybe_call_with_resilience(lambda: aws_utils.download_prefix_to_local(f"{s3_raw.rstrip('/')}/raw_banners", raw_banners, s3_opts, s3_conc), "s3_download")

                # If both downloads failed/returned falsy, fallback to local
                if not logos_download_ok and not banners_download_ok:
                    logger.warning("S3 downloads failed or empty for both logos and banners; falling back to local dirs")
                    _increment_fallback_metric()
                    use_s3 = False
                else:
                    # Process downloaded files
                    logos_paths = process_images_from_dir(raw_logos, proc_logos, "logo", overwrite, quality)
                    banners_paths = process_images_from_dir(raw_banners, proc_banners, "banner", overwrite, quality)
                    logos_count = len(logos_paths)
                    banners_count = len(banners_paths)

                    # Upload processed results if output S3 configured
                    if s3_proc and s3_proc.lower().startswith("s3://"):
                        upload_logos_ok = True
                        upload_banners_ok = True
                        if logos_count > 0:
                            upload_logos_ok = _maybe_call_with_resilience(lambda: aws_utils.upload_dir_to_s3(proc_logos, f"{s3_proc.rstrip('/')}/processed_logos", s3_opts, s3_conc), "s3_upload")
                        if banners_count > 0:
                            upload_banners_ok = _maybe_call_with_resilience(lambda: aws_utils.upload_dir_to_s3(proc_banners, f"{s3_proc.rstrip('/')}/processed_banners", s3_opts, s3_conc), "s3_upload")

                        status = "not_attempted"
                        if logos_count > 0 or banners_count > 0:
                            if upload_logos_ok and upload_banners_ok:
                                status = "success"
                            elif upload_logos_ok or upload_banners_ok:
                                status = "partial_success"
                            else:
                                status = "failure"
                        _audit_log("image.upload", s3_proc, status, {"logos_processed": logos_count, "logos_uploaded": bool(upload_logos_ok), "banners_processed": banners_count, "banners_uploaded": bool(upload_banners_ok)})
                        if status != "success":
                            logger.warning(f"S3 upload finished with status: {status}")
        except Exception:
            logger.exception("S3 workflow failed; falling back to local processing")
            _increment_fallback_metric()
            use_s3 = False

    if not use_s3:
        logger.info("Executing local processing workflow")
        logos_paths = process_images_from_dir(LOCAL_RAW_LOGOS, LOCAL_PROC_LOGOS, "logo", overwrite, quality)
        banners_paths = process_images_from_dir(LOCAL_RAW_BANNERS, LOCAL_PROC_BANNERS, "banner", overwrite, quality)
        logos_count = len(logos_paths)
        banners_count = len(banners_paths)

    logger.info(f"process_all_batch completed: logos={logos_count}, banners={banners_count}")
    return logos_count, banners_count


# --- Efficient single-file processor ---
def process_single(path_str: str, kind: str = "auto", overwrite: bool = True, quality: Optional[int] = None) -> Optional[Path]:
    try:
        src = Path(path_str).resolve()
    except Exception:
        logger.error(f"Invalid path provided to process_single: {path_str}")
        return None

    if not src.is_file():
        logger.error(f"process_single: path is not a file: {src}")
        _audit_log("image.single.process", str(src), "failure", {"reason": "not_a_file"})
        _metric_inc("image_processor.failed.single.not_a_file", 1)
        return None

    determined = kind
    img = None
    try:
        if determined == "auto":
            name_low = src.name.lower()
            if "logo" in name_low:
                determined = "logo"
            elif "banner" in name_low:
                determined = "banner"
            elif src.parent == LOCAL_RAW_LOGOS:
                determined = "logo"
            elif src.parent == LOCAL_RAW_BANNERS:
                determined = "banner"
            else:
                img = _safe_open_image(src)
                if img:
                    try:
                        is_logo = img.width <= 400 and img.height <= 400
                        determined = "logo" if is_logo else "banner"
                        logger.debug(f"Auto-detected kind={determined} for {src.name} size={img.size}")
                    finally:
                        try: img.close()
                        except Exception: pass
                else:
                    logger.warning(f"Could not open {src.name} for auto-detect; defaulting to 'logo'")
                    determined = "logo"

        dest = LOCAL_PROC_LOGOS if determined == "logo" else LOCAL_PROC_BANNERS
        result = _process_one_image(src, dest, determined, overwrite, quality)
        if result:
            logger.info(f"process_single success: {result}")
            return result
        else:
            logger.error(f"process_single failed for {src}")
            return None
    finally:
        if img is not None:
            try:
                img.close()
            except Exception:
                pass


# --- Health check ---
def health_check() -> Dict[str, Any]:
    settings = get_image_settings()
    report: Dict[str, Any] = {"status": "healthy", "checks": {}, "version": "image-processor-r1-diamond", "timestamp": time.time()}

    if not _PILLOW_AVAILABLE:
        report["checks"]["pillow"] = {"status": "unavailable"}
        report["status"] = "unhealthy"
    else:
        report["checks"]["pillow"] = {"status": "available"}

    s3_raw = (settings.S3_RAW_IMAGES_PATH or "").strip()
    if s3_raw and s3_raw.lower().startswith("s3://"):
        try:
            items = _maybe_call_with_resilience(lambda: aws_utils.list_s3_prefix(s3_raw, settings.S3_STORAGE_OPTIONS, settings.S3_CONCURRENCY), "s3_healthcheck")
            if items is False:
                report["checks"]["s3_raw_connectivity"] = {"status": "unavailable", "detail": "resilience_blocked"}
                report["status"] = "degraded"
            elif isinstance(items, list):
                report["checks"]["s3_raw_connectivity"] = {"status": "available", "items_found": len(items)}
            else:
                report["checks"]["s3_raw_connectivity"] = {"status": "unknown"}
        except Exception as e:
            report["checks"]["s3_raw_connectivity"] = {"status": "error", "detail": str(e)}
            report["status"] = "degraded"
    else:
        report["checks"]["s3_raw_connectivity"] = {"status": "not_configured"}

    # local dirs
    report["checks"]["local_dirs"] = {
        "raw_logos_exists": LOCAL_RAW_LOGOS.is_dir(),
        "raw_banners_exists": LOCAL_RAW_BANNERS.is_dir(),
        "proc_logos_exists": LOCAL_PROC_LOGOS.is_dir(),
        "proc_banners_exists": LOCAL_PROC_BANNERS.is_dir()
    }

    # final evaluation
    if report["status"] == "healthy":
        if not s3_raw and not (LOCAL_RAW_LOGOS.is_dir() or LOCAL_RAW_BANNERS.is_dir()):
            report["status"] = "unhealthy"

    return report


# --- CLI helpers ---
def configure_logging_from_settings(log_level_override: Optional[str] = None) -> None:
    try:
        level_str = (log_level_override or get_image_settings().LOG_LEVEL).upper()
    except Exception:
        level_str = "INFO"
    level = getattr(logging, level_str, logging.INFO)
    if not logging.getLogger().handlers:
        logging.basicConfig(level=level, format="%(asctime)s %(levelname)-8s [%(name)s] %(message)s", datefmt="%Y-%m-%d %H:%M:%S", stream=sys.stdout)
    logger.setLevel(level)


def cli_entry() -> None:
    parser = argparse.ArgumentParser(description="Image Processor (Diamond R-03)")
    parser.add_argument("--mode", choices=["batch", "single"], default="batch")
    parser.add_argument("--file", help="Source file path for single mode")
    parser.add_argument("--quality", type=int, help="Override WEBP quality")
    parser.add_argument("--overwrite", action="store_true", help="Overwrite existing outputs")
    parser.add_argument("--verbose", action="store_true", help="Enable debug logging")
    args = parser.parse_args()

    configure_logging_from_settings("DEBUG" if args.verbose else None)
    start = time.monotonic()
    exit_code = 0
    try:
        if args.mode == "batch":
            logger.info("Starting batch processing (CLI)")
            l, b = process_all_batch(overwrite=args.overwrite, quality=args.quality, verbose=args.verbose)
            logger.info(f"Batch done. Logos={l}, Banners={b}")
        elif args.mode == "single":
            if not args.file:
                logger.error("--file is required for single mode")
                exit_code = 2
            else:
                res = process_single(args.file, overwrite=args.overwrite, quality=args.quality)
                if res:
                    logger.info(f"Single processed -> {res}")
                else:
                    logger.error("Single processing failed")
                    exit_code = 1
    except Exception:
        logger.exception("CLI execution error")
        exit_code = 1
    finally:
        elapsed = time.monotonic() - start
        logger.info(f"CLI finished in {elapsed:.2f}s with exit_code={exit_code}")
        sys.exit(exit_code)


if __name__ == "__main__":
    cli_entry()


__all__ = [
    "process_images_from_dir",
    "process_all_batch",
    "process_single",
    "health_check",
    "get_image_settings",
    "reset_cached_image_settings",
    "LOCAL_PROC_LOGOS",
    "LOCAL_PROC_BANNERS",
    "METRIC_KEYS",
]
